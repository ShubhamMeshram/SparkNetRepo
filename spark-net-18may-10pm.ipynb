{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "829b4b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/share/aws/emr/emrfs/lib/slf4j-log4j12-1.7.12.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/share/aws/redshift/jdbc/redshift-jdbc42-1.2.37.1061.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/18 14:50:17 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=yarn appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "#sample boilerplate code to start\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sqlf\n",
    "from functools import reduce\n",
    "import marty.common.python.spark.misc.utils as misc_utils\n",
    "import marty.models.lib.utils as model_utils\n",
    "from marty.lib.job_manager import JobManager\n",
    "import datetime\n",
    "import os\n",
    "import pyspark.sql.functions as sqlf\n",
    "import pyspark.sql.types as sqlt\n",
    "import pyspark.storagelevel as storagelevel\n",
    "import yaml\n",
    "import marty.common.python.io.s3 as s3_io\n",
    "import marty.common.python.spark.data.stats as stat_utils\n",
    "import marty.models.lib.utils as model_utils\n",
    "conf = (SparkConf()\n",
    ".setAppName(\"MAR-1057\")\n",
    ".set(\"spark.ui.showConsoleProgress\", True)\n",
    ".set( \"spark.sql.crossJoin.enabled\" , \"true\" )\n",
    "# .set(\"spark.sql.parquet.fs.optimized.committer.optimization-enabled\", True)\n",
    "# .set(\"spark.driver.memory\", \"15g\")\n",
    "# .set(\"spark.executor.memory\", \"15g\")\n",
    "# .set(\"spark.storage.memoryFraction\", 0)\n",
    "# .setMaster('local[7]')\n",
    "# .setMaster(\"local[7]\")\n",
    "# .set(\"spark.yarn.scheduler.heartbeat.interval-ms\", 7200000)\n",
    "# .set(\"spark.executor.heartbeatInterval\", 7200000)\n",
    "# .set(\"spark.network.timeout\", 7200000)\n",
    "# .set(\"spark.storage.memoryFraction\", 0.1)\n",
    ")\n",
    "sc = SparkContext.getOrCreate()\n",
    "print(sc)\n",
    "config_path = 'marty/assignment/conf/config.yaml'\n",
    "job = JobManager(\"MAR-1057\", config_path=config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ca37c97",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Index_Key: string (nullable = true)\n",
      " |-- Former_Member: string (nullable = true)\n",
      " |-- Selection_Date: string (nullable = true)\n",
      " |-- Campaign_Name: string (nullable = true)\n",
      " |-- Wave: string (nullable = true)\n",
      " |-- Mail_House_Date: string (nullable = true)\n",
      " |-- Drop_Date: string (nullable = true)\n",
      " |-- in_Home_Date: string (nullable = true)\n",
      " |-- merkle_id: string (nullable = true)\n",
      " |-- cell_code: string (nullable = true)\n",
      " |-- individual_id: string (nullable = true)\n",
      " |-- household_id: string (nullable = true)\n",
      " |-- address_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_initial: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- suffix: string (nullable = true)\n",
      " |-- address1: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- zip_plus4: string (nullable = true)\n",
      " |-- crrt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=job.spark.read.format(\"csv\").option(\"header\",\"true\").option(\"sep\",\"|\").load(\"s3://memberacquisition-data-out-prod/assignment/preselection/slim/recent/BJs_Merkle_DP_DSProspect_INPUT_SELECTION_Campaign_20220426_23927659.txt\")\n",
    "#df = job.spark.csv(\"s3://memberacquisition-data-out-prod/assignment/preselection/slim/recent/BJs_Merkle_DP_DSProspect_INPUT_SELECTION_Campaign_20220426_23927659.txt\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fe216ba",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:======================================================>(510 + 2) / 512]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|first_name|\n",
      "+----------+\n",
      "|     ADELE|\n",
      "|       TIA|\n",
      "|      VERA|\n",
      "|    GLENNA|\n",
      "|     VIJAY|\n",
      "|      TONG|\n",
      "|    NORVEL|\n",
      "|     ELKIN|\n",
      "|   BELICIA|\n",
      "|  JOSELINO|\n",
      "|    JAYSON|\n",
      "|    CHELSI|\n",
      "|    DAMIEN|\n",
      "|  YULEIDYS|\n",
      "|  BEATRCIE|\n",
      "|     FENEL|\n",
      "|    JOVANY|\n",
      "|     LUNDA|\n",
      "|     ANANI|\n",
      "|      AHME|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select('first_name').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94372d5b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['address_id',\n",
       " 'carrier_route',\n",
       " 'merkle_id',\n",
       " 'zipadd',\n",
       " 'household_id',\n",
       " 'individual_id',\n",
       " 'first_name',\n",
       " 'middle_ini',\n",
       " 'last_name',\n",
       " 'geo_latitude',\n",
       " 'geo_longitude',\n",
       " 'mbr_rwd_typ',\n",
       " 'mbr_typ_cd',\n",
       " 'mbr_enrl_dt',\n",
       " 'mbr_exp_dt',\n",
       " 'mbr_rnwl_dt',\n",
       " 'mbr_mfi_amt',\n",
       " 'mbr_age',\n",
       " 'hh_mbr_sid',\n",
       " 'mbr_addr_type_flg',\n",
       " 'biz_mbr',\n",
       " 'biz_mbr_primary',\n",
       " 'qty_current_mbr_nohh',\n",
       " 'qty_past_mbr_nohh',\n",
       " 'deceased_indicator',\n",
       " 'prison_indicator',\n",
       " 'phone_nbr',\n",
       " 'mbr_stts_cd',\n",
       " 'mbr_ezr_stts_ind',\n",
       " 'current_mbr_address_id',\n",
       " 'past_mbr_cd',\n",
       " 'days_lapsed',\n",
       " 'street_addr',\n",
       " 'street_addr_2',\n",
       " 'city',\n",
       " 'state',\n",
       " 'zip_frst5_cd',\n",
       " 'zip_lst4_cd',\n",
       " 'addr_type_flg',\n",
       " 'hh_merkle_marketing_index',\n",
       " 'hh_merkle_adjusted_wealth_rating',\n",
       " 'hh_age_range_in_household',\n",
       " 'hh_merkle_buyer_rating',\n",
       " 'hh_merkle_responder_rating',\n",
       " 'hh_merkle_investment_rating',\n",
       " 'hh_merkle_donor_rating',\n",
       " 'hh_merkle_spending_velocity',\n",
       " 'hh_merkle_spending_volume',\n",
       " 'hh_inferred_card_holder',\n",
       " 'hh_inferred_premium_cardholder',\n",
       " 'hh_merkle_marketing_rank',\n",
       " 'hh_number_of_sources_velocity',\n",
       " 'hh_zip_plus4_riskbase_score',\n",
       " 'hh_merkle_marketing_index_last_qtr',\n",
       " 'hh_merkle_marketing_index_2_qtr_ago',\n",
       " 'hh_merkle_marketing_index_3_qtr_ago',\n",
       " 'hh_merkle_marketing_index_4_qtr_ago',\n",
       " 'hh_hotline_code',\n",
       " 'hh_apparel_accessories',\n",
       " 'hh_domestics',\n",
       " 'hh_food_and_beverage',\n",
       " 'hh_footwear',\n",
       " 'hh_seasonal_holiday_products',\n",
       " 'hh_dsf_confirm_flag',\n",
       " 'hh_dsf_cmra_flag',\n",
       " 'hh_dsf_delivery_type',\n",
       " 'hh_dsf_residence',\n",
       " 'hh_dsf_business',\n",
       " 'hh_dsf_drop_flag',\n",
       " 'hh_dsf_drop_count',\n",
       " 'hh_dsf_throwback',\n",
       " 'hh_dsf_seasonal',\n",
       " 'hh_dsf_vacant',\n",
       " 'hh_electronics',\n",
       " 'hh_health_or_natural_foods',\n",
       " 'hh_tennis',\n",
       " 'hh_boating_or_sailing',\n",
       " 'hh_camping_or_hiking',\n",
       " 'hh_skiing',\n",
       " 'hh_cable_tv',\n",
       " 'hh_tv_shoppers',\n",
       " 'hh_bicycling',\n",
       " 'hh_children_0_to_3_yrs',\n",
       " 'hh_children_4_to_7_yrs',\n",
       " 'hh_children_8_to_12_yrs',\n",
       " 'hh_children_13_to_18_yrs',\n",
       " 'hh_telephone',\n",
       " 'hh_ds918_do_not_call_natl',\n",
       " 'hh_ds918_do_not_call_state',\n",
       " 'hh_bank_card',\n",
       " 'hh_retail_card',\n",
       " 'hh_gas_or_oil_card',\n",
       " 'hh_finance_loan_store',\n",
       " 'hh_upscale_retail_card',\n",
       " 'hh_motorcycling_flag',\n",
       " 'hh_recreational_vehicles',\n",
       " 'hh_auto_interest',\n",
       " 'hh_collectors',\n",
       " 'hh_sewing_or_crafts',\n",
       " 'hh_equity_in_home_actual',\n",
       " 'hh_equity_in_home_ranges',\n",
       " 'hh_home_improvement_value_actual',\n",
       " 'hh_home_improvement_value_ranges',\n",
       " 'hh_land_value_actual',\n",
       " 'hh_land_value_ranges',\n",
       " 'hh_first_mortgage_amount_actual',\n",
       " 'hh_first_mortgage_amount_ranges',\n",
       " 'hh_sales_deed_category',\n",
       " 'hh_mortgage_term',\n",
       " 'hh_mortgage_loan_type',\n",
       " 'hh_mortgage_date_year',\n",
       " 'hh_mortgage_date_ranges',\n",
       " 'hh_second_mortgage_amount_actual',\n",
       " 'hh_second_mortgage_amount_ranges',\n",
       " 'hh_second_mortgage_type',\n",
       " 'hh_second_deed_type',\n",
       " 'hh_refinance_flag',\n",
       " 'hh_equity_loan_flag',\n",
       " 'hh_number_of_baths',\n",
       " 'hh_number_of_bedrooms',\n",
       " 'hh_year_built_actual',\n",
       " 'hh_year_built_ranges',\n",
       " 'hh_property_indicator',\n",
       " 'hh_square_footage_actual',\n",
       " 'hh_square_footage_ranges',\n",
       " 'hh_same_mailing_or_property_address',\n",
       " 'hh_absentee_owner',\n",
       " 'hh_bad_property_address',\n",
       " 'hh_air_conditioning',\n",
       " 'hh_fireplaces',\n",
       " 'hh_swimming_pool_indicator',\n",
       " 'hh_recording_date_actual',\n",
       " 'hh_recording_date_ranges',\n",
       " 'hh_sale_date_actual',\n",
       " 'hh_sale_date_ranges',\n",
       " 'hh_sale_price_actual',\n",
       " 'hh_sale_price_ranges',\n",
       " 'hh_probable_auto_clubs_membership',\n",
       " 'hh_probable_auto_loan_for_new_car',\n",
       " 'hh_probable_investing_in_mutual_fund',\n",
       " 'hh_probable_investing_in_stocks_and_bonds',\n",
       " 'hh_probable_ordering_through_phone',\n",
       " 'hh_probable_ordering_through_internet',\n",
       " 'hh_probable_purchasing_new_laptop_or_desktop',\n",
       " 'hh_probable_switching_auto_insurance_policy',\n",
       " 'hh_probable_purchasing_life_insurance',\n",
       " 'hh_probable_purchasing_529_college_saving_plan',\n",
       " 'hh_probable_opening_cd_or_mm_acct_in_12_mths',\n",
       " 'hh_probable_opening_checking_acct_in_12_mths',\n",
       " 'hh_probable_opening_annuity_acct_in_12_mths',\n",
       " 'hh_adjusted_net_worth',\n",
       " 'hh_probable_prescriptions_filled',\n",
       " 'hh_probable_purchasing_care_and_beauty_products',\n",
       " 'hh_probable_using_internet_for_general_entertainment',\n",
       " 'hh_probable_catalog_shopping_by_phone',\n",
       " 'hh_probable_green_products_buyer',\n",
       " 'hh_probable_book_influencer',\n",
       " 'hh_probable_gambling_enthusiast',\n",
       " 'hh_datasource_flag',\n",
       " 'hh_number_of_ds_sources',\n",
       " 'hh_address',\n",
       " 'hh_city',\n",
       " 'hh_state',\n",
       " 'hh_zip',\n",
       " 'hh_section_center_scf',\n",
       " 'hh_zip_plus4',\n",
       " 'hh_carrier_route',\n",
       " 'hh_fips_state_code',\n",
       " 'hh_fips_county_code',\n",
       " 'hh_delivery_point_barcode',\n",
       " 'hh_address_type',\n",
       " 'hh_apartment_flag',\n",
       " 'hh_new_record_flag',\n",
       " 'hh_time_zone',\n",
       " 'hh_dma_code',\n",
       " 'hh_geo_census_2010_tract',\n",
       " 'hh_geo_census_2010_block_group',\n",
       " 'hh_geo_msa_code',\n",
       " 'hh_rural_urban_county_size_code',\n",
       " 'hh_geo_lat_or_long_level',\n",
       " 'hh_geo_latitude',\n",
       " 'hh_geo_latitude_directional',\n",
       " 'hh_geo_longitude',\n",
       " 'hh_geo_longitude_directional',\n",
       " 'hh_individual_count',\n",
       " 'hh_new_to_ms_recency',\n",
       " 'hh_ms_id_age_rank',\n",
       " 'hh_census_cape_median_age',\n",
       " 'hh_census_cape_pop_0_to_17',\n",
       " 'hh_census_cape_prct_pop_18_to_99plus',\n",
       " 'hh_census_cape_prct_pop_65_to_99plus',\n",
       " 'hh_census_cape_prct_white',\n",
       " 'hh_census_cape_prct_black',\n",
       " 'hh_census_cape_prct_asian',\n",
       " 'hh_census_cape_prct_hispanic',\n",
       " 'hh_census_cape_persons_per_household',\n",
       " 'hh_census_cape_mean_household_size',\n",
       " 'hh_census_cape_prct_household_married',\n",
       " 'hh_census_cape_prct_household_under_18',\n",
       " 'hh_census_cape_prct_household_married_with_lessthan_18',\n",
       " 'hh_census_cape_prct_household_married_without_lessthan_18',\n",
       " 'hh_census_cape_prct_spanish_speaking',\n",
       " 'hh_census_cape_median_educ_attained_25plus',\n",
       " 'hh_census_cape_median_home_value',\n",
       " 'hh_census_cape_prct_mobile_home',\n",
       " 'hh_census_cape_median_housing_unit_age',\n",
       " 'hh_census_cape_prct_owner_occupied',\n",
       " 'hh_census_cape_prct_renter_occupied',\n",
       " 'hh_census_cape_ispsa',\n",
       " 'hh_census_cape_ispsa_decile',\n",
       " 'hh_census_cape_income_decile',\n",
       " 'hh_census_cape_median_household_income',\n",
       " 'hh_cell_phone',\n",
       " 'hh_video_camera',\n",
       " 'hh_vegetarian',\n",
       " 'hh_general_hobbies',\n",
       " 'hh_general_social_causes',\n",
       " 'hh_general_cultural_arts',\n",
       " 'hh_general_club_membership',\n",
       " 'hh_visual_corrections',\n",
       " 'hh_dwelling_type',\n",
       " 'hh_automobile',\n",
       " 'hh_automobile_count',\n",
       " 'hh_number_of_persons_in_living_unit',\n",
       " 'hh_wealth_rating',\n",
       " 'hh_age_field_type',\n",
       " 'hh_household_income_field_type',\n",
       " 'hh_marital_status_field_type',\n",
       " 'hh_presence_of_children_field_type',\n",
       " 'hh_homeowner_field_type',\n",
       " 'hh_length_of_residence_field_type',\n",
       " 'hh_occupation_field_type',\n",
       " 'hh_home_value_field_type',\n",
       " 'hh_loan_to_value_field_type',\n",
       " 'hh_education_field_type',\n",
       " 'hh_number_of_total_rooms_field_type',\n",
       " 'hh_number_of_children_field_type',\n",
       " 'hh_computer_or_internet_field_type',\n",
       " 'hh_stocks_or_bonds_field_type',\n",
       " 'hh_mutual_funds_field_type',\n",
       " 'hh_money_market_funds_field_type',\n",
       " 'hh_real_estate_field_type',\n",
       " 'hh_domestic_travel_field_type',\n",
       " 'hh_foreign_travel_field_type',\n",
       " 'hh_cruises_field_type',\n",
       " 'hh_golf_field_type',\n",
       " 'hh_fitness_or_exercise_field_type',\n",
       " 'hh_grandparent_field_type',\n",
       " 'hh_diet_or_weight_loss_field_type',\n",
       " 'hh_cooking_field_type',\n",
       " 'hh_dog_owner_field_type',\n",
       " 'hh_cat_owner_field_type',\n",
       " 'hh_hunting_or_fishing_field_type',\n",
       " 'hh_book_buyers_field_type',\n",
       " 'hh_home_decorating_field_type',\n",
       " 'hh_gardening_field_type',\n",
       " 'hh_video_games_field_type',\n",
       " 'hh_non_retail_shopper_field_type',\n",
       " 'hh_music_field_type',\n",
       " 'hh_mail_buyer_field_type',\n",
       " 'hh_sweepstakes_or_gambling_field_type',\n",
       " 'hh_donor_field_type',\n",
       " 'hh_walking_or_running_field_type',\n",
       " 'hh_travel_field_type',\n",
       " 'hh_pet_owner_field_type',\n",
       " 'hh_internet_buyer_field_type',\n",
       " 'hh_veterans_field_type',\n",
       " 'hh_age',\n",
       " 'hh_household_income',\n",
       " 'hh_marital_status',\n",
       " 'hh_presence_of_children',\n",
       " 'hh_homeowner',\n",
       " 'hh_length_of_residence',\n",
       " 'hh_occupation',\n",
       " 'hh_home_value',\n",
       " 'hh_loan_to_value',\n",
       " 'hh_education',\n",
       " 'hh_number_of_total_rooms',\n",
       " 'hh_number_of_children',\n",
       " 'hh_computer_or_internet',\n",
       " 'hh_stocks_or_bonds',\n",
       " 'hh_mutual_funds',\n",
       " 'hh_money_market_funds',\n",
       " 'hh_real_estate',\n",
       " 'hh_domestic_travel',\n",
       " 'hh_foreign_travel',\n",
       " 'hh_cruises',\n",
       " 'hh_golf',\n",
       " 'hh_fitness_or_exercise',\n",
       " 'hh_grandparent',\n",
       " 'hh_diet_or_weight_loss',\n",
       " 'hh_cooking',\n",
       " 'hh_dog_owner',\n",
       " 'hh_cat_owner',\n",
       " 'hh_hunting_or_fishing',\n",
       " 'hh_book_buyers',\n",
       " 'hh_home_decorating',\n",
       " 'hh_gardening',\n",
       " 'hh_video_games',\n",
       " 'hh_non_retail_shopper',\n",
       " 'hh_music',\n",
       " 'hh_mail_buyer',\n",
       " 'hh_sweepstakes_or_gambling',\n",
       " 'hh_donor',\n",
       " 'hh_walking_or_running',\n",
       " 'hh_travel',\n",
       " 'hh_pet_owner',\n",
       " 'hh_internet_buyer',\n",
       " 'hh_veterans',\n",
       " 'hh_presence_of_grandchildren_12_years_or_younger',\n",
       " 'hh_soho_business',\n",
       " 'hh_truck_owner',\n",
       " 'hh_credit_card',\n",
       " 'hh_credit_card_new_issue',\n",
       " 'hh_travel_card',\n",
       " 'hh_new_credit_range',\n",
       " 'hh_number_credit_lines',\n",
       " 'hh_credit_ranges',\n",
       " 'hh_contribute_animal',\n",
       " 'hh_contribute_childrens_causes',\n",
       " 'hh_contribute_environment',\n",
       " 'hh_contributors',\n",
       " 'hh_contribute_health',\n",
       " 'hh_contribute_political',\n",
       " 'hh_contribute_religious',\n",
       " 'hh_contribute_veterans',\n",
       " 'hh_contributor_index',\n",
       " 'hh_auto_motorcycle_racing',\n",
       " 'hh_auto_parts_repair_accessories_work',\n",
       " 'hh_career_minded',\n",
       " 'hh_children_interests',\n",
       " 'hh_do_it_yourselfer',\n",
       " 'hh_entertainment_enthusiast',\n",
       " 'hh_family',\n",
       " 'hh_great_outdoors',\n",
       " 'hh_health_beauty',\n",
       " 'hh_health_living',\n",
       " 'hh_high_tech_leader',\n",
       " 'hh_living_luxury_life',\n",
       " 'hh_magazine',\n",
       " 'hh_movie',\n",
       " 'hh_owns_satellite_tv',\n",
       " 'hh_tv_sports',\n",
       " 'hh_travel_business',\n",
       " 'hh_travel_personal',\n",
       " 'hh_travel_vacation',\n",
       " 'hh_hobby_photography',\n",
       " 'hh_hobby_wine_appreciation',\n",
       " 'hh_hobby_woodworking',\n",
       " 'hh_genre_music_type_classical',\n",
       " 'hh_genre_music_type_country',\n",
       " 'hh_genre_music_type_rock_n_roll',\n",
       " 'hh_reading_computer',\n",
       " 'hh_reading_cooking_or_culinary',\n",
       " 'hh_reading_fashion',\n",
       " 'hh_reading_medical_or_health',\n",
       " 'hh_reading_natural_health_remedies',\n",
       " 'hh_reading_science_fiction',\n",
       " 'hh_baseball',\n",
       " 'hh_basketball',\n",
       " 'hh_football',\n",
       " 'hh_hockey',\n",
       " 'hh_nascar',\n",
       " 'hh_running',\n",
       " 'hh_snow_skiing',\n",
       " 'hh_walking',\n",
       " 'hh_sports',\n",
       " 'hh_female_merchandise_buyer',\n",
       " 'hh_special_foods_buyer',\n",
       " 'hh_mail_order_childrens_products',\n",
       " 'hh_mail_order_clothing',\n",
       " 'hh_mail_order_cosmetics',\n",
       " 'hh_mail_order_dvd',\n",
       " 'hh_membership_warehouse',\n",
       " 'hh_value_hunter',\n",
       " 'hh_general_self_improvement',\n",
       " 'hh_bible_devotional',\n",
       " 'hh_reading_general_fiction',\n",
       " 'hh_nations_heritage',\n",
       " 'hh_science_technology',\n",
       " 'hh_current_affairs_politics_news',\n",
       " 'hh_mail_order_gifts_other',\n",
       " 'hh_toys_board_games_kids_books',\n",
       " 'hh_genre_music_jazz_rb_soft_rock',\n",
       " 'hh_sports_reading_memorabilia',\n",
       " 'hh_reading_astrology_people_entertainment',\n",
       " 'hh_credit_card_premium',\n",
       " 'hh_lc_club_sports',\n",
       " 'hh_lc_traditionalist',\n",
       " 'hh_lc_professionalist',\n",
       " 'hh_lc_investor',\n",
       " 'hh_lc_audio_visual',\n",
       " 'hh_lc_campgrounder',\n",
       " 'hh_lc_intelligentsia',\n",
       " 'hh_lc_mechanic',\n",
       " 'hh_lc_reader',\n",
       " 'hh_lc_chiphead',\n",
       " 'hh_lc_home_garden',\n",
       " 'hh_lc_triathlete',\n",
       " 'hh_lc_connoisseur',\n",
       " 'hh_lc_ecologist',\n",
       " 'hh_lc_tv_guide',\n",
       " 'hh_lc_handicrafts',\n",
       " 'hh_lc_field_stream',\n",
       " 'hh_ld_athletic',\n",
       " 'hh_ld_bluechip',\n",
       " 'hh_ld_cultural',\n",
       " 'hh_ld_domestic',\n",
       " 'hh_ld_goodlife',\n",
       " 'hh_ld_technology',\n",
       " 'hh_address_quality',\n",
       " 'hh_recipient_reliability_code',\n",
       " 'hh_merchandise_upscale_buyers',\n",
       " 'hh_merchandise_male_buyers',\n",
       " 'hh_merchandise_female_buyers',\n",
       " 'hh_merchandise_crafts_or_hobbies',\n",
       " 'hh_merchandise_specialty_food',\n",
       " 'hh_merchandise_gifts_or_gadgets',\n",
       " 'hh_merchandise_general',\n",
       " 'hh_magazines_family_or_general',\n",
       " 'hh_magazines_female',\n",
       " 'hh_magazines_male_or_sports',\n",
       " 'hh_publications_religious',\n",
       " 'hh_publications_do_it_yourself',\n",
       " 'hh_publications_news_and_financial',\n",
       " 'hh_publications_photography',\n",
       " 'hh_publications_opportune_or_contest',\n",
       " 'hh_contributor_religious',\n",
       " 'hh_contributor_political',\n",
       " 'hh_contributor_health_or_institutional',\n",
       " 'hh_contributor_general',\n",
       " 'hh_mail_responders_misc',\n",
       " 'hh_odds_and_ends',\n",
       " 'hh_in_the_market_predictor',\n",
       " 'hh_household_composition',\n",
       " 'hh_new_car_model',\n",
       " 'hh_used_car_model',\n",
       " 'hh_mosaic_household_level_segmentation',\n",
       " 'hh_mosaic_global_household_level_segmentation',\n",
       " 'hh_equity_rate_type',\n",
       " 'hh_equity_term',\n",
       " 'hh_equity_loan_type',\n",
       " 'hh_deed_date_of_refinance_loan',\n",
       " 'hh_refinance_amt_in_thousands',\n",
       " 'hh_refinance_rate_type',\n",
       " 'hh_refinance_loan_type',\n",
       " 'hh_phone_pander_flag',\n",
       " 'hh_contributes_to_charities',\n",
       " 'hh_cbsa_code',\n",
       " 'hh_cbsa_type',\n",
       " 'hh_mor_mail_order_responder_deduped_count',\n",
       " 'hh_mor_mail_order_responder_non_deduped_count',\n",
       " 'hh_home_heat_indicator',\n",
       " 'hh_home_building_construction',\n",
       " 'hh_deed_date_of_equity_loan',\n",
       " 'hh_move_date',\n",
       " 'hh_date',\n",
       " 'hh_day',\n",
       " 'hh_month',\n",
       " 'hh_year',\n",
       " 'ind_bj_current_customer_in',\n",
       " 'ind_frst_nm',\n",
       " 'ind_mdl_nm',\n",
       " 'ind_lst_nm',\n",
       " 'ind_suffix',\n",
       " 'ind_dob',\n",
       " 'ind_age',\n",
       " 'ind_gendr',\n",
       " 'ind_type',\n",
       " 'ind_marital_status',\n",
       " 'ind_inferred_age',\n",
       " 'ind_title',\n",
       " 'ind_education',\n",
       " 'ind_occupation',\n",
       " 'ind_do_not_mail',\n",
       " 'ind_deceased_flg',\n",
       " 'ind_sflg',\n",
       " 'ind_household_rank',\n",
       " 'ind_source_cnt',\n",
       " 'ind_inferred_marital_status',\n",
       " 'ind_inferred_education',\n",
       " 'ind_ethnic_cd',\n",
       " 'ind_religion_cd',\n",
       " 'ind_group_ethnic_cd',\n",
       " 'ind_spoken_language_cd',\n",
       " 'ind_origin_cd',\n",
       " 'ind_hisp_asian_assim_index',\n",
       " 'ind_political_party',\n",
       " 'ind_boater_flag',\n",
       " 'ind_date',\n",
       " 'ind_day',\n",
       " 'ind_month',\n",
       " 'ind_year',\n",
       " 'age',\n",
       " 'current_mbr_zipadd',\n",
       " 'current_mbr_merkle_id',\n",
       " 'current_mbr',\n",
       " 'addr_dm_inflight',\n",
       " 'addr_dm_last_12m',\n",
       " 'addr_recency_last_dm',\n",
       " 'responder_1_yr',\n",
       " 'responder_2_yr',\n",
       " 'responder_3_yr',\n",
       " 'responder_prev_m1',\n",
       " 'responder_prev_m2',\n",
       " 'responder_prev_m3',\n",
       " 'date',\n",
       " 'day',\n",
       " 'month',\n",
       " 'year',\n",
       " 'initial_selection_date',\n",
       " 'force_in',\n",
       " 'force_in_reasons',\n",
       " 'utuc',\n",
       " 'pp_resp_score',\n",
       " 'pros_resp_score',\n",
       " 'resp_score',\n",
       " 'pp_resp_adj_score',\n",
       " 'pros_resp_adj_score',\n",
       " 'resp_adj_score',\n",
       " 'pros_ltv_1yr_score',\n",
       " 'pp_ltv_1yr_score',\n",
       " 'ltv_1yr_score',\n",
       " 'resp_adj_ltv_1yr_score',\n",
       " 'mbr_sid',\n",
       " 'past_mbr',\n",
       " 'offer_type',\n",
       " 'offer_selected_array',\n",
       " 'offer_selected_ind',\n",
       " 'offer_eligible',\n",
       " 'source',\n",
       " 'zip',\n",
       " 'crrt',\n",
       " 'route_dna_geo_latitude',\n",
       " 'route_dna_geo_longitude',\n",
       " 'distance_bjs',\n",
       " 'distance_costco',\n",
       " 'distance_costco_business',\n",
       " 'distance_sams',\n",
       " 'distance_nearest_competitor',\n",
       " 'bjs_adv',\n",
       " 'route_in_pta',\n",
       " 'route_dna_state',\n",
       " 'cr_total_hh',\n",
       " 'cr_total_individual',\n",
       " 'cr_sum_dm_3m',\n",
       " 'cr_sum_dm_12m',\n",
       " 'cr_sum_dm_last1',\n",
       " 'cr_sum_dm_last2',\n",
       " 'cr_sum_dm_last3',\n",
       " 'cr_avg_mbr_mfi',\n",
       " 'cr_sum_trips_12m',\n",
       " 'cr_sum_spend_12m',\n",
       " 'cr_count_shopping_mbr_12m',\n",
       " 'cr_avg_mbr_trip_12m',\n",
       " 'cr_avg_mbr_spend_12m',\n",
       " 'club',\n",
       " 'cr_count_addr',\n",
       " 'cr_count_addr_pta',\n",
       " 'cr_count_addr_current_mbr',\n",
       " 'cr_count_addr_past_mbr',\n",
       " 'cr_count_addr_prospect',\n",
       " 'cr_sum_responder_1y',\n",
       " 'cr_sum_responder_2y',\n",
       " 'cr_sum_responder_3y',\n",
       " 'cr_sum_responder_prev_m1',\n",
       " 'cr_sum_responder_prev_m2',\n",
       " 'cr_sum_responder_prev_m3',\n",
       " 'cr_avg_responder_3m',\n",
       " 'cr_sum_qty_current_members',\n",
       " 'cr_sum_qty_past_members',\n",
       " 'cr_sum_mlty_fmly_dwlng_cnt',\n",
       " 'cr_sum_lapsed_trial_mbr',\n",
       " 'cr_sum_lapsed_paid_mbr',\n",
       " 'cr_response_rate_1y',\n",
       " 'cr_response_rate_2y',\n",
       " 'cr_response_rate_3y',\n",
       " 'cr_response_rate_3m',\n",
       " 'cr_mbr_penetration_rate',\n",
       " 'cr_mbr_addr_penetration_rate',\n",
       " 'cr_past_mbr_addr_penetration_rate',\n",
       " 'cr_prospect_addr_penetration_rate',\n",
       " 'cr_avg_age',\n",
       " 'cr_avg_income',\n",
       " 'cr_avg_homevalue',\n",
       " 'cr_avg_med_hv',\n",
       " 'cr_avg_networth',\n",
       " 'cr_pct_sfh',\n",
       " 'cr_pct_apt',\n",
       " 'cr_pct_married',\n",
       " 'cr_pct_withkids',\n",
       " 'cr_response_rate_3y_decile',\n",
       " 'route_dna_date',\n",
       " 'route_dna_day',\n",
       " 'route_dna_month',\n",
       " 'route_dna_year',\n",
       " 'cr_agg_pros_resp_score',\n",
       " 'cr_agg_pros_resp_ltv_1yr_score',\n",
       " 'cr_agg_pros_resp_decile',\n",
       " 'cr_agg_pros_resp_ltv_1yr_decile',\n",
       " 'exclusion_type',\n",
       " 'include_flag',\n",
       " 'index_key']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full = job.read(\"preselected_population_recent\")\n",
    "df_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7d33025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host '619ca0ea68ebaa001753c9b0.mockapi.io'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "url = \"https://619ca0ea68ebaa001753c9b0.mockapi.io/evaliation/dataengineer/jr/v1/users\"\n",
    "url = \"https://619ca0ea68ebaa001753c9b0.mockapi.io/evaliation/dataengineer/jr/v1/messages\"\n",
    "\n",
    "r=requests.get(url,verify=False)\n",
    "en= r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0714d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:================================================>     (345 + 5) / 387]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|_corrupt_record|\n",
      "+---------------+\n",
      "|\"Not found\"    |\n",
      "+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "json_rdd = sc.parallelize([r.text])\n",
    "\n",
    "df = job.spark.read.json(json_rdd)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d74a0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e981d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinging the API https://619ca0ea68ebaa001753c9b0.mockapi.io/evaluation/dataengineer/jr/v1/users\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host '619ca0ea68ebaa001753c9b0.mockapi.io'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n",
      "/home/hadoop/.local/lib/python3.7/site-packages/urllib3/connectionpool.py:1050: InsecureRequestWarning: Unverified HTTPS request is being made to host '619ca0ea68ebaa001753c9b0.mockapi.io'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API responded with status code 200\n",
      "Pinging the API https://619ca0ea68ebaa001753c9b0.mockapi.io/evaluation/dataengineer/jr/v1/messages\n",
      "API responded with status code 200\n",
      "Generating JSON at /home/hadoop/usr_api.json\n",
      "Generating JSON at /home/hadoop/msg_api.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as f\n",
    "#from pyspark.sql.functions import explode_outer\n",
    "#from pyspark.sql.functions import split\n",
    "#from pyspark.sql.functions import col\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "import requests\n",
    "\n",
    "config = ReadConfigFile(\"spark-net.yaml\")\n",
    "config = add_dates_to_paths(config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8999bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting write operation for user dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting write operation for user_attributes dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting write operation for user_subscription dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting write operation for msg dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting write operation for analytics_op dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "usr_df = job.spark.read.format('json').option(\"multiLine\", \"true\").load(config[\"paths\"][\"usr_api_raw\"]['path'])\n",
    "msg_df = job.spark.read.format('json').option(\"multiLine\", \"true\").load(config[\"paths\"][\"msg_api_raw\"]['path'])\n",
    "\n",
    "usr_df=usr_df.withColumnRenamed(\"id\", \"userId\")\n",
    "msg_df=msg_df.withColumnRenamed(\"id\", \"msgId\")\n",
    "\n",
    "\n",
    "usr_df=ConvertStringToTimeStamp(usr_df,'createdAt')\n",
    "usr_df=ConvertStringToTimeStamp(usr_df,'updatedAt')\n",
    "msg_df=ConvertStringToTimeStamp(msg_df,'createdAt')\n",
    "\n",
    "#create user_sub and user_attr table\n",
    "user_attr_df=usr_df.select(\"userId\",\"profile.*\")#.show(500,False)\n",
    "user_sub_df = usr_df.select(\"userId\",explode_outer(\"subscription\")).select(\"userId\",\"col.*\")#.show(500,False)\n",
    "\n",
    "w = Window.partitionBy('userId')\n",
    "user_sub_df_slim = user_sub_df.withColumn('maxB', f.max('startDate').over(w)).where(f.col('startDate') == f.col('maxB')).drop('maxB')\n",
    "\n",
    "usr_df = usr_df.drop(\"profile\")\n",
    "usr_df = usr_df.drop(\"subscription\")\n",
    "#adding feature column -domain\n",
    "usr_df = usr_df.withColumn(\"domain\",split(split(col(\"email\"), \"@\").getItem(1),\".com\").getItem(0))\n",
    "\n",
    "#temp table for qurying\n",
    "msg_df.registerTempTable(\"msg_df\")\n",
    "usr_df.registerTempTable(\"usr_df\")\n",
    "user_attr_df.registerTempTable(\"user_attr_df\")\n",
    "user_sub_df.registerTempTable(\"user_sub_df\")\n",
    "user_sub_df_slim.registerTempTable(\"user_sub_df_slim\")\n",
    "\n",
    "#write all 4 tables to S3\n",
    "write(usr_df,\"user\",config)\n",
    "write(user_attr_df,\"user_attributes\",config)\n",
    "write(user_sub_df,\"user_subscription\",config)\n",
    "write(msg_df,\"msg\",config)\n",
    "GenerateAnalyticsOutput(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5a5fe77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|msg_cnt|\n",
      "+-------+\n",
      "|20     |\n",
      "+-------+\n",
      "\n",
      "+----------+\n",
      "|lonely_usr|\n",
      "+----------+\n",
      "|4         |\n",
      "+----------+\n",
      "\n",
      "+------------+\n",
      "|actv_sub_cnt|\n",
      "+------------+\n",
      "|4           |\n",
      "+------------+\n",
      "\n",
      "+----------------+\n",
      "|freemiuim_userId|\n",
      "+----------------+\n",
      "|2               |\n",
      "|5               |\n",
      "|1               |\n",
      "|3               |\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#3.1\n",
    "q=\"SELECT count(*) as msg_cnt FROM msg_df\"\n",
    "job.spark.sql(q).show(500,False)\n",
    "#3.2\n",
    "q=\"\"\"SELECT distinct(userId) as lonely_usr FROM usr_df except SELECT distinct(receiverId) FROM msg_df\"\"\"\n",
    "job.spark.sql(q).show(500,False)\n",
    "#3.3\n",
    "q=\"SELECT count(*) as actv_sub_cnt from user_sub_df where status =='Active' \"\n",
    "job.spark.sql(q).show(500,False)\n",
    "#3.4\n",
    "q=\"\"\"SELECT distinct(userId) as freemiuim_userId from user_sub_df_slim where status ==\"Active\" intersect SELECT distinct(senderId) as sub_cnt from msg_df\"\"\"\n",
    "job.spark.sql(q).show(500,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "a3470f23",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+------------------------+------------------------+------------------------+--------+\n",
      "|userId        |amount|createdAt               |endDate                 |startDate               |status  |\n",
      "+--------------+------+------------------------+------------------------+------------------------+--------+\n",
      "|*** Masked ***|23.78 |2021-11-24T14:36:18.895Z|2022-07-13T09:14:04.001Z|2021-11-24T12:57:48.724Z|Active  |\n",
      "|*** Masked ***|89.71 |2021-11-24T14:40:24.257Z|2022-11-23T12:41:28.319Z|2021-11-24T11:22:33.265Z|Active  |\n",
      "|*** Masked ***|43.18 |2021-11-24T16:58:46.581Z|2022-09-15T06:05:59.630Z|2021-11-24T05:12:49.301Z|Active  |\n",
      "|*** Masked ***|3.51  |2021-11-24T02:07:08.482Z|2021-12-10T20:22:36.132Z|2021-11-24T12:47:33.246Z|Inactive|\n",
      "|*** Masked ***|88.60 |2021-11-23T18:57:20.540Z|2022-03-03T09:47:26.916Z|2021-11-24T18:04:41.908Z|Active  |\n",
      "+--------------+------+------------------------+------------------------+------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_sub_df_slim.withColumn(\"userId\", lit(\"*** Masked ***\")).show(500,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2172669b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'ProUkE0wwl8q2JR7J2DDO6NMkwWnGcufWaB5sVFjL40='\n"
     ]
    },
    {
     "ename": "InvalidToken",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/cryptography/fernet.py\u001b[0m in \u001b[0;36m_get_unverified_token_data\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlsafe_b64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinascii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/base64.py\u001b[0m in \u001b[0;36murlsafe_b64decode\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_urlsafe_decode_translation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/base64.py\u001b[0m in \u001b[0;36mb64decode\u001b[0;34m(s, altchars, validate)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mbinascii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Non-base64 digit found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbinascii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma2b_base64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Incorrect padding",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidToken\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25382/195030437.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFernet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdecrypted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecrypt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencrypted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/cryptography/fernet.py\u001b[0m in \u001b[0;36mdecrypt\u001b[0;34m(self, token, ttl)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecrypt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mtimestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFernet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_unverified_token_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mttl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mtime_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/cryptography/fernet.py\u001b[0m in \u001b[0;36m_get_unverified_token_data\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlsafe_b64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinascii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidToken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0x80\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidToken\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "812e9d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "84f69518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'gAAAAABihRuceiqrf7KGyGyzlHBrYLfY3yly6aWZ6wGNis9aogjrbGqeONZyQxSjhcjyAELaCbsvz5bbXKeU4ODeCkClItJl2MC3ZswtjM5b1KnFLfowG5k='\n",
      "shubham is my name\n"
     ]
    }
   ],
   "source": [
    "def generate_key():\n",
    "    \"\"\"\n",
    "    Generates a key and save it into a file\n",
    "    \"\"\"\n",
    "    key = Fernet.generate_key()\n",
    "    with open(\"secret.key\", \"wb\") as key_file:\n",
    "        key_file.write(key)\n",
    "generate_key()\n",
    "\n",
    "def load_key():\n",
    "    \"\"\"\n",
    "    Load the previously generated key\n",
    "    \"\"\"\n",
    "    return open(\"secret.key\", \"rb\").read()\n",
    "\n",
    "def ConvertStringToTimeStamp(spark_df,ts_col):\n",
    "    spark_df = spark_df.withColumn(\"temp_ts_col\",spark_df[ts_col].cast(TimestampType()))\n",
    "    spark_df = spark_df.drop(ts_col)\n",
    "    spark_df=spark_df.withColumnRenamed(\"temp_ts_col\", ts_col)\n",
    "    return spark_df\n",
    "\n",
    "def encrypt_message(message):\n",
    "    \"\"\"\n",
    "    Encrypts a message\n",
    "    \"\"\"\n",
    "    #key = load_key()\n",
    "    #print(key)\n",
    "    import cryptography\n",
    "\n",
    "    key = b'SgC_PuQOhFINn8XkKhnWMOKtWTSl8RnUXchTbeCz1XS='\n",
    "\n",
    "    encoded_message = message.encode()\n",
    "    f = Fernet(key)\n",
    "    encrypted_message = f.encrypt(encoded_message)\n",
    "\n",
    "    print(encrypted_message)\n",
    "    return encrypted_message\n",
    "    \n",
    "def decrypt_message(encrypted_message):\n",
    "    \"\"\"\n",
    "    Decrypts an encrypted message\n",
    "    \"\"\"\n",
    "    #key = load_key()\n",
    "    key = b'SgC_PuQOhFINn8XkKhnWMOKtWTSl8RnUXchTbeCz1XS='\n",
    "\n",
    "    f = Fernet(key)\n",
    "    decrypted_message = f.decrypt(encrypted_message)\n",
    "\n",
    "    print(decrypted_message.decode())\n",
    "\n",
    "aa= encrypt_message(\"shubham is my name\")\n",
    "decrypt_message(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9cf40acf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://nexus.bjs.com/repository/pip-registry/simple\n",
      "Requirement already satisfied: cryptography in /home/hadoop/.local/lib/python3.7/site-packages (37.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/hadoop/.local/lib/python3.7/site-packages (from cryptography) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /home/hadoop/.local/lib/python3.7/site-packages (from cffi>=1.12->cryptography) (2.21)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://nexus.bjs.com/repository/pip-registry/simple\n",
      "Requirement already satisfied: paramiko in /home/hadoop/.local/lib/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: six in /home/hadoop/.local/lib/python3.7/site-packages (from paramiko) (1.16.0)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /home/hadoop/.local/lib/python3.7/site-packages (from paramiko) (3.2.2)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /home/hadoop/.local/lib/python3.7/site-packages (from paramiko) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /home/hadoop/.local/lib/python3.7/site-packages (from paramiko) (37.0.2)\n",
      "Requirement already satisfied: cffi>=1.1 in /home/hadoop/.local/lib/python3.7/site-packages (from bcrypt>=3.1.3->paramiko) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /home/hadoop/.local/lib/python3.7/site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install cryptography\n",
    "!pip3 install paramiko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "098e9393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python: No module named pip\r\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install cryptography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c19e19bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/18 16:15:28 WARN TaskSetManager: Lost task 0.0 in stage 86.0 (TID 4206) (ip-10-13-53-162.bjw2k.asg executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/worker.py\", line 588, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "ModuleNotFoundError: No module named 'cryptography'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:907)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:359)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/05/18 16:15:29 ERROR TaskSetManager: Task 0 in stage 86.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/worker.py\", line 588, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'cryptography'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25382/562472674.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0musr_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fname_en\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencrypt_message_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"firstname\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcryptography\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \"\"\"\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/worker.py\", line 588, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/worker.py\", line 447, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/worker.py\", line 249, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/worker.py\", line 69, in read_command\n    command = serializer._read_with_length(file)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n    return self.loads(obj)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1652487863319_0004/container_1652487863319_0004_01_000002/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n    return pickle.loads(obj, encoding=encoding)\nModuleNotFoundError: No module named 'cryptography'\n"
     ]
    }
   ],
   "source": [
    "import cryptography\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "encrypt_message_fn = udf(lambda x:encrypt_message(x),StringType() )\n",
    "r= usr_df.withColumn(\"fname_en\",encrypt_message_fn(col(\"firstname\")))\n",
    "import cryptography\n",
    "r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "29811332",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25382/3015171933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencrypt_message_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"firstname\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "encrypt_message_fn(col(\"firstname\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
